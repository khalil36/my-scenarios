{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Random Forest Algorithm on Sonar Dataset\n",
        "from random import seed\n",
        "from random import randrange\n",
        "from csv import reader\n",
        "from math import sqrt\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Random Forest\n",
        "Decision trees can suffer from high variance which makes their results fragile to the specific\n",
        "training data used. Building multiple models from samples of your training data, called bagging,\n",
        "can reduce this variance, but the trees are highly correlated.\n",
        "\n",
        "Random Forest is an extension of bagging that in addition to building trees based on multiple\n",
        "samples of your training data, it also constrains the features that can be used to build the trees,\n",
        "forcing trees to be different. This, in turn, can give a lift in performance. In this tutorial, you\n",
        "will discover how to implement the Random Forest algorithm from scratch in Python. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sonar Dataset\n",
        "In this tutorial we will use the Sonar Dataset. This dataset involves the discrimination between\n",
        "mines and rocks. The baseline performance on the problem is approximately 53%.\n",
        "\n",
        "We will use the helper function load_csv() to load the file, str column to float() to convert string numbers to floats and\n",
        "str column to int() to convert the class column to integer values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load a CSV file\n",
        "def load_csv(filename):\n",
        "\tdataset = list()\n",
        "\twith open(filename, 'r') as file:\n",
        "\t\tcsv_reader = reader(file)\n",
        "\t\tfor row in csv_reader:\n",
        "\t\t\tif not row:\n",
        "\t\t\t\tcontinue\n",
        "\t\t\tdataset.append(row)\n",
        "\treturn dataset\n",
        "\n",
        "# Convert string column to float\n",
        "def str_column_to_float(dataset, column):\n",
        "\tfor row in dataset:\n",
        "\t\trow[column] = float(row[column].strip())\n",
        "\n",
        "# Convert string column to integer\n",
        "def str_column_to_int(dataset, column):\n",
        "\tclass_values = [row[column] for row in dataset]\n",
        "\tunique = set(class_values)\n",
        "\tlookup = dict()\n",
        "\tfor i, value in enumerate(unique):\n",
        "\t\tlookup[value] = i\n",
        "\tfor row in dataset:\n",
        "\t\trow[column] = lookup[row[column]]\n",
        "\treturn lookup\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will use k-fold cross-validation to estimate the performance of the learned model on\n",
        "unseen data. This means that we will construct and evaluate k models and estimate the\n",
        "performance as the mean model error. Classification accuracy will be used to evaluate each\n",
        "model. These behaviors are provided in the cross validation split(), accuracy metric()\n",
        "and evaluate algorithm() helper functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Split a dataset into k folds\n",
        "def cross_validation_split(dataset, n_folds):\n",
        "\tdataset_split = list()\n",
        "\tdataset_copy = list(dataset)\n",
        "\tfold_size = int(len(dataset) / n_folds)\n",
        "\tfor _ in range(n_folds):\n",
        "\t\tfold = list()\n",
        "\t\twhile len(fold) < fold_size:\n",
        "\t\t\tindex = randrange(len(dataset_copy))\n",
        "\t\t\tfold.append(dataset_copy.pop(index))\n",
        "\t\tdataset_split.append(fold)\n",
        "\treturn dataset_split\n",
        "\n",
        "# Calculate accuracy percentage\n",
        "def accuracy_metric(actual, predicted):\n",
        "\tcorrect = 0\n",
        "\tfor i in range(len(actual)):\n",
        "\t\tif actual[i] == predicted[i]:\n",
        "\t\t\tcorrect += 1\n",
        "\treturn correct / float(len(actual)) * 100.0\n",
        "\n",
        "# Evaluate an algorithm using a cross validation split\n",
        "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
        "\tfolds = cross_validation_split(dataset, n_folds)\n",
        "\tscores = list()\n",
        "\tfor fold in folds:\n",
        "\t\ttrain_set = list(folds)\n",
        "\t\ttrain_set.remove(fold)\n",
        "\t\ttrain_set = sum(train_set, [])\n",
        "\t\ttest_set = list()\n",
        "\t\tfor row in fold:\n",
        "\t\t\trow_copy = list(row)\n",
        "\t\t\ttest_set.append(row_copy)\n",
        "\t\t\trow_copy[-1] = None\n",
        "\t\tpredicted = algorithm(train_set, test_set, *args)\n",
        "\t\tactual = [row[-1] for row in fold]\n",
        "\t\taccuracy = accuracy_metric(actual, predicted)\n",
        "\t\tscores.append(accuracy)\n",
        "\treturn scores\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We need function test _split() to split a dataset into groups, gini index() to evaluate a split point, get split() to find an\n",
        "optimal split point, to terminal(), split() and build tree() used to create a single decision\n",
        "tree, predict() to make a prediction with a decision tree and the subsample() function\n",
        "described in the previous step to make a subsample of the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Split a dataset based on an attribute and an attribute value\n",
        "def test_split(index, value, dataset):\n",
        "\tleft, right = list(), list()\n",
        "\tfor row in dataset:\n",
        "\t\tif row[index] < value:\n",
        "\t\t\tleft.append(row)\n",
        "\t\telse:\n",
        "\t\t\tright.append(row)\n",
        "\treturn left, right\n",
        "\n",
        "# Calculate the Gini index for a split dataset\n",
        "def gini_index(groups, classes):\n",
        "\t# count all samples at split point\n",
        "\tn_instances = float(sum([len(group) for group in groups]))\n",
        "\t# sum weighted Gini index for each group\n",
        "\tgini = 0.0\n",
        "\tfor group in groups:\n",
        "\t\tsize = float(len(group))\n",
        "\t\t# avoid divide by zero\n",
        "\t\tif size == 0:\n",
        "\t\t\tcontinue\n",
        "\t\tscore = 0.0\n",
        "\t\t# score the group based on the score for each class\n",
        "\t\tfor class_val in classes:\n",
        "\t\t\tp = [row[-1] for row in group].count(class_val) / size\n",
        "\t\t\tscore += p * p\n",
        "\t\t# weight the group score by its relative size\n",
        "\t\tgini += (1.0 - score) * (size / n_instances)\n",
        "\treturn gini\n",
        "\n",
        "# Select the best split point for a dataset\n",
        "def get_split(dataset, n_features):\n",
        "\tclass_values = list(set(row[-1] for row in dataset))\n",
        "\tb_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
        "\tfeatures = list()\n",
        "\twhile len(features) < n_features:\n",
        "\t\tindex = randrange(len(dataset[0])-1)\n",
        "\t\tif index not in features:\n",
        "\t\t\tfeatures.append(index)\n",
        "\tfor index in features:\n",
        "\t\tfor row in dataset:\n",
        "\t\t\tgroups = test_split(index, row[index], dataset)\n",
        "\t\t\tgini = gini_index(groups, class_values)\n",
        "\t\t\tif gini < b_score:\n",
        "\t\t\t\tb_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
        "\treturn {'index':b_index, 'value':b_value, 'groups':b_groups}\n",
        "\n",
        "# Create a terminal node value\n",
        "def to_terminal(group):\n",
        "\toutcomes = [row[-1] for row in group]\n",
        "\treturn max(set(outcomes), key=outcomes.count)\n",
        "\n",
        "# Create child splits for a node or make terminal\n",
        "def split(node, max_depth, min_size, n_features, depth):\n",
        "\tleft, right = node['groups']\n",
        "\tdel(node['groups'])\n",
        "\t# check for a no split\n",
        "\tif not left or not right:\n",
        "\t\tnode['left'] = node['right'] = to_terminal(left + right)\n",
        "\t\treturn\n",
        "\t# check for max depth\n",
        "\tif depth >= max_depth:\n",
        "\t\tnode['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
        "\t\treturn\n",
        "\t# process left child\n",
        "\tif len(left) <= min_size:\n",
        "\t\tnode['left'] = to_terminal(left)\n",
        "\telse:\n",
        "\t\tnode['left'] = get_split(left, n_features)\n",
        "\t\tsplit(node['left'], max_depth, min_size, n_features, depth+1)\n",
        "\t# process right child\n",
        "\tif len(right) <= min_size:\n",
        "\t\tnode['right'] = to_terminal(right)\n",
        "\telse:\n",
        "\t\tnode['right'] = get_split(right, n_features)\n",
        "\t\tsplit(node['right'], max_depth, min_size, n_features, depth+1)\n",
        "\n",
        "# Build a decision tree\n",
        "def build_tree(train, max_depth, min_size, n_features):\n",
        "\troot = get_split(train, n_features)\n",
        "\tsplit(root, max_depth, min_size, n_features, 1)\n",
        "\treturn root\n",
        "\n",
        "# Make a prediction with a decision tree\n",
        "def predict(node, row):\n",
        "\tif row[node['index']] < node['value']:\n",
        "\t\tif isinstance(node['left'], dict):\n",
        "\t\t\treturn predict(node['left'], row)\n",
        "\t\telse:\n",
        "\t\t\treturn node['left']\n",
        "\telse:\n",
        "\t\tif isinstance(node['right'], dict):\n",
        "\t\t\treturn predict(node['right'], row)\n",
        "\t\telse:\n",
        "\t\t\treturn node['right']\n",
        "\n",
        "# Create a random subsample from the dataset with replacement\n",
        "def subsample(dataset, ratio):\n",
        "\tsample = list()\n",
        "\tn_sample = round(len(dataset) * ratio)\n",
        "\twhile len(sample) < n_sample:\n",
        "\t\tindex = randrange(len(dataset))\n",
        "\t\tsample.append(dataset[index])\n",
        "\treturn sample\n",
        "\n",
        "# Make a prediction with a list of bagged trees\n",
        "def bagging_predict(trees, row):\n",
        "\tpredictions = [predict(tree, row) for tree in trees]\n",
        "\treturn max(set(predictions), key=predictions.count)\n",
        "\n",
        "# Random Forest Algorithm\n",
        "def random_forest(train, test, max_depth, min_size, sample_size, n_trees, n_features):\n",
        "\ttrees = list()\n",
        "\tfor _ in range(n_trees):\n",
        "\t\tsample = subsample(train, sample_size)\n",
        "\t\ttree = build_tree(sample, max_depth, min_size, n_features)\n",
        "\t\ttrees.append(tree)\n",
        "\tpredictions = [bagging_predict(trees, row) for row in test]\n",
        "\treturn(predictions)\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A k value of 5 was used for cross-validation, giving each fold 208/5 = 41.6 or just over 40\n",
        "records to be evaluated upon each iteration. Deep trees were constructed with a max depth of\n",
        "10 and a minimum number of training rows at each node of 1. Samples of the training dataset\n",
        "were created with the same size as the original dataset, which is a default expectation for the\n",
        "Random Forest algorithm.\n",
        "\n",
        "The number of features considered at each split point was set to suare root num_features which is equal to 7.74\n",
        "rounded to 7 features. A suite of 3 different numbers of trees were evaluated for comparison,\n",
        "showing the increasing skill as more trees are added. Running the example prints the scores for\n",
        "each fold and mean score for each configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Test the random forest algorithm on sonar dataset\n",
        "seed(2)\n",
        "# load and prepare data\n",
        "filename = 'sonar.all-data.csv'\n",
        "dataset = load_csv(filename)\n",
        "# convert string attributes to integers\n",
        "for i in range(0, len(dataset[0])-1):\n",
        "\tstr_column_to_float(dataset, i)\n",
        "# convert class column to integers\n",
        "str_column_to_int(dataset, len(dataset[0])-1)\n",
        "# evaluate algorithm\n",
        "n_folds = 5\n",
        "max_depth = 10\n",
        "min_size = 1\n",
        "sample_size = 1.0\n",
        "n_features = int(sqrt(len(dataset[0])-1))\n",
        "for n_trees in [1, 5, 10]:\n",
        "\tscores = evaluate_algorithm(dataset, random_forest, n_folds, max_depth, min_size, sample_size, n_trees, n_features)\n",
        "\tprint('Trees: %d' % n_trees)\n",
        "\tprint('Scores: %s' % scores)\n",
        "\tprint('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}