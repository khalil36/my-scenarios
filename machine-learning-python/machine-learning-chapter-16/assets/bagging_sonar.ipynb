{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Bagging Algorithm on the Sonar dataset\n",
        "from random import seed\n",
        "from random import randrange\n",
        "from csv import reader\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sonar Case Study\n",
        "In this section, we will apply the Bagging algorithm to the Sonar dataset. The example assumes that a CSV copy of the dataset is in the \n",
        "current working directory with the file name sonar.all-data.csv.\n",
        "\n",
        "The dataset is first loaded, the string values converted to numeric and the output column\n",
        "is converted from strings to the integer values of 0 to 1. This is achieved with helper functions load csv(), str column to float() and str column to int() to load and prepare the\n",
        "dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load a CSV file\n",
        "def load_csv(filename):\n",
        "\tdataset = list()\n",
        "\twith open(filename, 'r') as file:\n",
        "\t\tcsv_reader = reader(file)\n",
        "\t\tfor row in csv_reader:\n",
        "\t\t\tif not row:\n",
        "\t\t\t\tcontinue\n",
        "\t\t\tdataset.append(row)\n",
        "\treturn dataset\n",
        "\n",
        "# Convert string column to float\n",
        "def str_column_to_float(dataset, column):\n",
        "\tfor row in dataset:\n",
        "\t\trow[column] = float(row[column].strip())\n",
        "\n",
        "# Convert string column to integer\n",
        "def str_column_to_int(dataset, column):\n",
        "\tclass_values = [row[column] for row in dataset]\n",
        "\tunique = set(class_values)\n",
        "\tlookup = dict()\n",
        "\tfor i, value in enumerate(unique):\n",
        "\t\tlookup[value] = i\n",
        "\tfor row in dataset:\n",
        "\t\trow[column] = lookup[row[column]]\n",
        "\treturn lookup"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will use k-fold cross-validation to estimate the performance of the learned model on\n",
        "unseen data. This means that we will construct and evaluate k models and estimate the\n",
        "performance as the mean model error. Classification accuracy will be used to evaluate each\n",
        "model. These behaviors are provided in the cross validation split(), accuracy metric()\n",
        "and evaluate algorithm() helper functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Split a dataset into k folds\n",
        "def cross_validation_split(dataset, n_folds):\n",
        "\tdataset_split = list()\n",
        "\tdataset_copy = list(dataset)\n",
        "\tfold_size = int(len(dataset) / n_folds)\n",
        "\tfor _ in range(n_folds):\n",
        "\t\tfold = list()\n",
        "\t\twhile len(fold) < fold_size:\n",
        "\t\t\tindex = randrange(len(dataset_copy))\n",
        "\t\t\tfold.append(dataset_copy.pop(index))\n",
        "\t\tdataset_split.append(fold)\n",
        "\treturn dataset_split\n",
        "\n",
        "# Calculate accuracy percentage\n",
        "def accuracy_metric(actual, predicted):\n",
        "\tcorrect = 0\n",
        "\tfor i in range(len(actual)):\n",
        "\t\tif actual[i] == predicted[i]:\n",
        "\t\t\tcorrect += 1\n",
        "\treturn correct / float(len(actual)) * 100.0\n",
        "\n",
        "# Evaluate an algorithm using a cross validation split\n",
        "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
        "\tfolds = cross_validation_split(dataset, n_folds)\n",
        "\tscores = list()\n",
        "\tfor fold in folds:\n",
        "\t\ttrain_set = list(folds)\n",
        "\t\ttrain_set.remove(fold)\n",
        "\t\ttrain_set = sum(train_set, [])\n",
        "\t\ttest_set = list()\n",
        "\t\tfor row in fold:\n",
        "\t\t\trow_copy = list(row)\n",
        "\t\t\ttest_set.append(row_copy)\n",
        "\t\t\trow_copy[-1] = None\n",
        "\t\tpredicted = algorithm(train_set, test_set, *args)\n",
        "\t\tactual = [row[-1] for row in fold]\n",
        "\t\taccuracy = accuracy_metric(actual, predicted)\n",
        "\t\tscores.append(accuracy)\n",
        "\treturn scores\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will also use an implementation of the Classification and Regression Trees (CART)\n",
        "algorithm adapted for bagging with the helper functions from Chapter 11 including test split()\n",
        "to split a dataset into groups, gini index() to evaluate a split point, get split() to find an\n",
        "optimal split point, to terminal(), split() and build tree() used to create a single decision\n",
        "tree, predict() to make a prediction with a decision tree and the subsample() function\n",
        "described in the previous step to make a subsample of the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Split a dataset based on an attribute and an attribute value\n",
        "def test_split(index, value, dataset):\n",
        "\tleft, right = list(), list()\n",
        "\tfor row in dataset:\n",
        "\t\tif row[index] < value:\n",
        "\t\t\tleft.append(row)\n",
        "\t\telse:\n",
        "\t\t\tright.append(row)\n",
        "\treturn left, right\n",
        "\n",
        "# Calculate the Gini index for a split dataset\n",
        "def gini_index(groups, classes):\n",
        "\t# count all samples at split point\n",
        "\tn_instances = float(sum([len(group) for group in groups]))\n",
        "\t# sum weighted Gini index for each group\n",
        "\tgini = 0.0\n",
        "\tfor group in groups:\n",
        "\t\tsize = float(len(group))\n",
        "\t\t# avoid divide by zero\n",
        "\t\tif size == 0:\n",
        "\t\t\tcontinue\n",
        "\t\tscore = 0.0\n",
        "\t\t# score the group based on the score for each class\n",
        "\t\tfor class_val in classes:\n",
        "\t\t\tp = [row[-1] for row in group].count(class_val) / size\n",
        "\t\t\tscore += p * p\n",
        "\t\t# weight the group score by its relative size\n",
        "\t\tgini += (1.0 - score) * (size / n_instances)\n",
        "\treturn gini\n",
        "\n",
        "# Select the best split point for a dataset\n",
        "def get_split(dataset):\n",
        "\tclass_values = list(set(row[-1] for row in dataset))\n",
        "\tb_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
        "\tfor index in range(len(dataset[0])-1):\n",
        "\t\tfor row in dataset:\n",
        "\t\t# for i in range(len(dataset)):\n",
        "\t\t# \trow = dataset[randrange(len(dataset))]\n",
        "\t\t\tgroups = test_split(index, row[index], dataset)\n",
        "\t\t\tgini = gini_index(groups, class_values)\n",
        "\t\t\tif gini < b_score:\n",
        "\t\t\t\tb_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
        "\treturn {'index':b_index, 'value':b_value, 'groups':b_groups}\n",
        "\n",
        "# Create a terminal node value\n",
        "def to_terminal(group):\n",
        "\toutcomes = [row[-1] for row in group]\n",
        "\treturn max(set(outcomes), key=outcomes.count)\n",
        "\n",
        "# Create child splits for a node or make terminal\n",
        "def split(node, max_depth, min_size, depth):\n",
        "\tleft, right = node['groups']\n",
        "\tdel(node['groups'])\n",
        "\t# check for a no split\n",
        "\tif not left or not right:\n",
        "\t\tnode['left'] = node['right'] = to_terminal(left + right)\n",
        "\t\treturn\n",
        "\t# check for max depth\n",
        "\tif depth >= max_depth:\n",
        "\t\tnode['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
        "\t\treturn\n",
        "\t# process left child\n",
        "\tif len(left) <= min_size:\n",
        "\t\tnode['left'] = to_terminal(left)\n",
        "\telse:\n",
        "\t\tnode['left'] = get_split(left)\n",
        "\t\tsplit(node['left'], max_depth, min_size, depth+1)\n",
        "\t# process right child\n",
        "\tif len(right) <= min_size:\n",
        "\t\tnode['right'] = to_terminal(right)\n",
        "\telse:\n",
        "\t\tnode['right'] = get_split(right)\n",
        "\t\tsplit(node['right'], max_depth, min_size, depth+1)\n",
        "\n",
        "# Build a decision tree\n",
        "def build_tree(train, max_depth, min_size):\n",
        "\troot = get_split(train)\n",
        "\tsplit(root, max_depth, min_size, 1)\n",
        "\treturn root\n",
        "\n",
        "# Make a prediction with a decision tree\n",
        "def predict(node, row):\n",
        "\tif row[node['index']] < node['value']:\n",
        "\t\tif isinstance(node['left'], dict):\n",
        "\t\t\treturn predict(node['left'], row)\n",
        "\t\telse:\n",
        "\t\t\treturn node['left']\n",
        "\telse:\n",
        "\t\tif isinstance(node['right'], dict):\n",
        "\t\t\treturn predict(node['right'], row)\n",
        "\t\telse:\n",
        "\t\t\treturn node['right']\n",
        "\n",
        "# Create a random subsample from the dataset with replacement\n",
        "def subsample(dataset, ratio):\n",
        "\tsample = list()\n",
        "\tn_sample = round(len(dataset) * ratio)\n",
        "\twhile len(sample) < n_sample:\n",
        "\t\tindex = randrange(len(dataset))\n",
        "\t\tsample.append(dataset[index])\n",
        "\treturn sample"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A new function named bagging_predict() is developed that is responsible for making a\n",
        "prediction with each decision tree and combining the predictions into a single return value. This\n",
        "is achieved by selecting the most common prediction from the list of predictions made by the\n",
        "bagged trees."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Make a prediction with a list of bagged trees\n",
        "def bagging_predict(trees, row):\n",
        "\tpredictions = [predict(tree, row) for tree in trees]\n",
        "\treturn max(set(predictions), key=predictions.count)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, a new function named bagging() is developed that is responsible for creating the\n",
        "samples of the training dataset, training a decision tree on each, then making predictions on the\n",
        "test dataset using the list of bagged trees. The complete example is listed below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Bootstrap Aggregation Algorithm\n",
        "def bagging(train, test, max_depth, min_size, sample_size, n_trees):\n",
        "\ttrees = list()\n",
        "\tfor _ in range(n_trees):\n",
        "\t\tsample = subsample(train, sample_size)\n",
        "\t\ttree = build_tree(sample, max_depth, min_size)\n",
        "\t\ttrees.append(tree)\n",
        "\tpredictions = [bagging_predict(trees, row) for row in test]\n",
        "\treturn(predictions)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A k value of 5 was used for cross-validation, giving each fold 208/5 = 41.6 or just over 40\n",
        "records to be evaluated upon each iteration.\n",
        "\n",
        "Deep trees were constructed with a max depth of 6 and a minimum number of training rows\n",
        "at each node of 2. Samples of the training dataset were created with 50% the size of the original\n",
        "dataset. This was to force some variety in the dataset subsamples used to train each tree. The\n",
        "default for bagging is to have the size of sample datasets match the size of the original training\n",
        "dataset.\n",
        "\n",
        "A series of 4 different numbers of trees were evaluated to show the behavior of the algorithm.\n",
        "The accuracy from each fold and the mean accuracy for each configuration are printed. We can\n",
        "see a trend of some minor lift in performance as the number of trees is increased."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Test bagging on the sonar dataset\n",
        "seed(1)\n",
        "# load and prepare data\n",
        "filename = 'sonar.all-data.csv'\n",
        "dataset = load_csv(filename)\n",
        "# convert string attributes to integers\n",
        "for i in range(len(dataset[0])-1):\n",
        "\tstr_column_to_float(dataset, i)\n",
        "# convert class column to integers\n",
        "str_column_to_int(dataset, len(dataset[0])-1)\n",
        "# evaluate algorithm\n",
        "n_folds = 5\n",
        "max_depth = 6\n",
        "min_size = 2\n",
        "sample_size = 0.50\n",
        "for n_trees in [1, 5, 10, 50]:\n",
        "\tscores = evaluate_algorithm(dataset, bagging, n_folds, max_depth, min_size, sample_size, n_trees)\n",
        "\tprint('Trees: %d' % n_trees)\n",
        "\tprint('Scores: %s' % scores)\n",
        "\tprint('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}