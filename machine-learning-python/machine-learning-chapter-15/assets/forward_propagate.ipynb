{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Example of forward propagating input\n",
        "from math import exp\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Forward-Propagate\n",
        "We can calculate an output from a neural network by propagating an input signal through\n",
        "each layer until the output layer outputs its values. We call this forward-propagation. It is the\n",
        "technique we will need to generate predictions during training that will need to be corrected,\n",
        "and it is the method we will need after the network is trained to make predictions on new data.\n",
        "\n",
        "We can break forward-propagation down into three parts:\n",
        "1. Neuron Activation.\n",
        "2. Neuron Transfer.\n",
        "3. Forward-Propagation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Calculate neuron activation for an input\n",
        "def activate(weights, inputs):\n",
        "\tactivation = weights[-1]\n",
        "\tfor i in range(len(weights)-1):\n",
        "\t\tactivation += weights[i] * inputs[i]\n",
        "\treturn activation\n",
        "\n",
        "# Transfer neuron activation\n",
        "def transfer(activation):\n",
        "\treturn 1.0 / (1.0 + exp(-activation))\n",
        "\n",
        "# Forward propagate input to a network output\n",
        "def forward_propagate(network, row):\n",
        "\tinputs = row\n",
        "\tfor layer in network:\n",
        "\t\tnew_inputs = []\n",
        "\t\tfor neuron in layer:\n",
        "\t\t\tactivation = activate(neuron['weights'], inputs)\n",
        "\t\t\tneuron['output'] = transfer(activation)\n",
        "\t\t\tnew_inputs.append(neuron['output'])\n",
        "\t\tinputs = new_inputs\n",
        "\treturn inputs\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's test out the forward-propagation of our network.\n",
        "We define our network inline with one hidden neuron that expects 2 input values and an output\n",
        "layer with two neurons."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# test forward propagation\n",
        "network = [[{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}],\n",
        "\t\t[{'weights': [0.2550690257394217, 0.49543508709194095]}, {'weights': [0.4494910647887381, 0.651592972722763]}]]\n",
        "row = [1, 0, None]\n",
        "output = forward_propagate(network, row)\n",
        "print(output)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Running the above example propagates the input pattern [1, 0] and produces an output value that\n",
        "is printed. Because the output layer has two neurons, we get a list of two numbers as output.\n",
        "The actual output values are just nonsense for now, but next, we will start to learn how to\n",
        "make the weights in the neurons more useful."
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}