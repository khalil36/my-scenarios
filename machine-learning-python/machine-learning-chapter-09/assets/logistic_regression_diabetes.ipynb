{
  "nbformat_minor": 1, 
  "nbformat": 4, 
  "cells": [
    {
      "source": [
        "# Logistic Regression on Diabetes Dataset\n", 
        "from random import seed\n", 
        "from random import randrange\n", 
        "from csv import reader\n", 
        "from math import exp\n"
      ], 
      "cell_type": "code", 
      "execution_count": null, 
      "outputs": [], 
      "metadata": {}
    }, 
    {
      "source": [
        "In this section, we will train a logistic regression model using stochastic gradient descent on\n", 
        "the diabetes dataset. The example uses dataset present in the current\n", 
        "working directory with the filename pima-indians-diabetes.csv<br />\n", 
        "\n", 
        "The dataset is first loaded, the string values converted to numeric and each column is\n", 
        "normalized to values in the range of 0 to 1. This is achieved with the helper functions load csv()\n", 
        "and str column to float() to load and prepare the dataset and dataset minmax() and\n", 
        "normalize dataset() to normalize it."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "# Load a CSV file\n", 
        "def load_csv(filename):\n", 
        "\tdataset = list()\n", 
        "\twith open(filename, 'r') as file:\n", 
        "\t\tcsv_reader = reader(file)\n", 
        "\t\tfor row in csv_reader:\n", 
        "\t\t\tif not row:\n", 
        "\t\t\t\tcontinue\n", 
        "\t\t\tdataset.append(row)\n", 
        "\treturn dataset\n", 
        "\n", 
        "# Convert string column to float\n", 
        "def str_column_to_float(dataset, column):\n", 
        "\tfor row in dataset:\n", 
        "\t\trow[column] = float(row[column].strip())\n", 
        "\n", 
        "# Find the min and max values for each column\n", 
        "def dataset_minmax(dataset):\n", 
        "\tminmax = list()\n", 
        "\tfor i in range(len(dataset[0])):\n", 
        "\t\tcol_values = [row[i] for row in dataset]\n", 
        "\t\tvalue_min = min(col_values)\n", 
        "\t\tvalue_max = max(col_values)\n", 
        "\t\tminmax.append([value_min, value_max])\n", 
        "\treturn minmax\n", 
        "\n", 
        "# Rescale dataset columns to the range 0-1\n", 
        "def normalize_dataset(dataset, minmax):\n", 
        "\tfor row in dataset:\n", 
        "\t\tfor i in range(len(row)):\n", 
        "\t\t\trow[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n"
      ], 
      "cell_type": "code", 
      "execution_count": null, 
      "outputs": [], 
      "metadata": {}
    }, 
    {
      "source": [
        "We will use k-fold cross-validation to estimate the performance of the learned model on unseen\n", 
        "data. This means that we will construct and evaluate k models and estimate the performance\n", 
        "as the mean model performance. Classification accuracy will be used to evaluate each model.\n", 
        "These behaviors are provided in the cross validation split(), accuracy metric() and\n", 
        "evaluate algorithm() helper functions."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "# Split a dataset into k folds\n", 
        "def cross_validation_split(dataset, n_folds):\n", 
        "\tdataset_split = list()\n", 
        "\tdataset_copy = list(dataset)\n", 
        "\tfold_size = int(len(dataset) / n_folds)\n", 
        "\tfor _ in range(n_folds):\n", 
        "\t\tfold = list()\n", 
        "\t\twhile len(fold) < fold_size:\n", 
        "\t\t\tindex = randrange(len(dataset_copy))\n", 
        "\t\t\tfold.append(dataset_copy.pop(index))\n", 
        "\t\tdataset_split.append(fold)\n", 
        "\treturn dataset_split\n", 
        "\n", 
        "# Calculate accuracy percentage\n", 
        "def accuracy_metric(actual, predicted):\n", 
        "\tcorrect = 0\n", 
        "\tfor i in range(len(actual)):\n", 
        "\t\tif actual[i] == predicted[i]:\n", 
        "\t\t\tcorrect += 1\n", 
        "\treturn correct / float(len(actual)) * 100.0\n", 
        "\n", 
        "# Evaluate an algorithm using a cross validation split\n", 
        "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n", 
        "\tfolds = cross_validation_split(dataset, n_folds)\n", 
        "\tscores = list()\n", 
        "\tfor fold in folds:\n", 
        "\t\ttrain_set = list(folds)\n", 
        "\t\ttrain_set.remove(fold)\n", 
        "\t\ttrain_set = sum(train_set, [])\n", 
        "\t\ttest_set = list()\n", 
        "\t\tfor row in fold:\n", 
        "\t\t\trow_copy = list(row)\n", 
        "\t\t\ttest_set.append(row_copy)\n", 
        "\t\t\trow_copy[-1] = None\n", 
        "\t\tpredicted = algorithm(train_set, test_set, *args)\n", 
        "\t\tactual = [row[-1] for row in fold]\n", 
        "\t\taccuracy = accuracy_metric(actual, predicted)\n", 
        "\t\tscores.append(accuracy)\n", 
        "\treturn scores\n", 
        "\n", 
        "# Make a prediction with coefficients\n", 
        "def predict(row, coefficients):\n", 
        "\tyhat = coefficients[0]\n", 
        "\tfor i in range(len(row)-1):\n", 
        "\t\tyhat += coefficients[i + 1] * row[i]\n", 
        "\treturn 1.0 / (1.0 + exp(-yhat))\n", 
        "\n", 
        "# Estimate logistic regression coefficients using stochastic gradient descent\n", 
        "def coefficients_sgd(train, l_rate, n_epoch):\n", 
        "\tcoef = [0.0 for i in range(len(train[0]))]\n", 
        "\tfor _ in range(n_epoch):\n", 
        "\t\tfor row in train:\n", 
        "\t\t\tyhat = predict(row, coef)\n", 
        "\t\t\terror = row[-1] - yhat\n", 
        "\t\t\tcoef[0] = coef[0] + l_rate * error * yhat * (1.0 - yhat)\n", 
        "\t\t\tfor i in range(len(row)-1):\n", 
        "\t\t\t\tcoef[i + 1] = coef[i + 1] + l_rate * error * yhat * (1.0 - yhat) * row[i]\n", 
        "\treturn coef\n", 
        "\n", 
        "# Logistic Regression Algorithm With Stochastic Gradient Descent\n", 
        "def logistic_regression(train, test, l_rate, n_epoch):\n", 
        "\tpredictions = list()\n", 
        "\tcoef = coefficients_sgd(train, l_rate, n_epoch)\n", 
        "\tfor row in test:\n", 
        "\t\tyhat = predict(row, coef)\n", 
        "\t\tyhat = round(yhat)\n", 
        "\t\tpredictions.append(yhat)\n", 
        "\treturn(predictions)\n", 
        "\n"
      ], 
      "cell_type": "code", 
      "execution_count": null, 
      "outputs": [], 
      "metadata": {}
    }, 
    {
      "source": [
        "We will use the predict() and coefficients sgd() functions created above and a new\n", 
        "logistic regression() function to train the model."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "# Test the logistic regression algorithm on the diabetes dataset\n", 
        "seed(1)\n", 
        "# load and prepare data\n", 
        "filename = 'pima-indians-diabetes.csv'\n", 
        "dataset = load_csv(filename)\n", 
        "for i in range(len(dataset[0])):\n", 
        "\tstr_column_to_float(dataset, i)\n", 
        "# normalize\n", 
        "minmax = dataset_minmax(dataset)\n", 
        "normalize_dataset(dataset, minmax)\n", 
        "# evaluate algorithm\n", 
        "n_folds = 5\n", 
        "l_rate = 0.1\n", 
        "n_epoch = 100\n", 
        "scores = evaluate_algorithm(dataset, logistic_regression, n_folds, l_rate, n_epoch)\n", 
        "print('Scores: %s' % scores)\n", 
        "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
      ], 
      "cell_type": "code", 
      "execution_count": null, 
      "outputs": [], 
      "metadata": {}
    }, 
    {
      "source": [
        "A k value of 5 was used for cross-validation, giving each fold 768/5 = 153.6 or just over 150\n", 
        "records to be evaluated upon each iteration. A learning rate of 0.1 and 100 training epochs were\n", 
        "chosen with a little experimentation. You can try your own configurations and see if you can\n", 
        "beat my score.\n", 
        "Running this example prints the scores for each of the 5 cross-validation folds, then prints\n", 
        "the mean classification accuracy. We can see that the accuracy is about 77%, higher than the\n", 
        "baseline value of 65%."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }
  ], 
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3", 
      "name": "python3", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "3.6.1", 
      "pygments_lexer": "ipython3", 
      "codemirror_mode": {
        "version": 3, 
        "name": "ipython"
      }
    }, 
    "anaconda-cloud": {}
  }
}