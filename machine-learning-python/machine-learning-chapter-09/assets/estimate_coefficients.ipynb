{
  "nbformat_minor": 1, 
  "nbformat": 4, 
  "cells": [
    {
      "source": [
        "# Example of estimating coefficients\n", 
        "from math import exp\n"
      ], 
      "cell_type": "code", 
      "execution_count": null, 
      "outputs": [], 
      "metadata": {}
    }, 
    {
      "source": [
        "## Logistic Regression\n", 
        "Logistic regression is named for the function used at the core of the method, the logistic function.\n", 
        "Logistic regression uses an equation as the representation, very much like linear regression.\n", 
        "Input values (X) are combined linearly using weights or coefficient values to predict an output\n", 
        "value (y). A key difference from linear regression is that the output value being modeled is a\n", 
        "binary value (0 or 1) rather than a numeric value."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "# Make a prediction with coefficients\n", 
        "def predict(row, coefficients):\n", 
        "\tyhat = coefficients[0]\n", 
        "\tfor i in range(len(row)-1):\n", 
        "\t\tyhat += coefficients[i + 1] * row[i]\n", 
        "\treturn 1.0 / (1.0 + exp(-yhat))\n"
      ], 
      "cell_type": "code", 
      "execution_count": null, 
      "outputs": [], 
      "metadata": {}
    }, 
    {
      "source": [
        "## Estimating Coefficients\n", 
        "We can estimate the coefficient values for our training data using stochastic gradient descent.\n", 
        "Stochastic gradient descent requires two parameters:\n", 
        "- **Learning Rate:** Used to limit the amount each coefficient is corrected each time it is\n", 
        "updated.\n", 
        "- **Epochs:** The number of times to run through the training data while updating the\n", 
        "coefficients."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "# Estimate logistic regression coefficients using stochastic gradient descent\n", 
        "def coefficients_sgd(train, l_rate, n_epoch):\n", 
        "\tcoef = [0.0 for i in range(len(train[0]))]\n", 
        "\tfor epoch in range(n_epoch):\n", 
        "\t\tsum_error = 0\n", 
        "\t\tfor row in train:\n", 
        "\t\t\tyhat = predict(row, coef)\n", 
        "\t\t\terror = row[-1] - yhat\n", 
        "\t\t\tsum_error += error**2\n", 
        "\t\t\tcoef[0] = coef[0] + l_rate * error * yhat * (1.0 - yhat)\n", 
        "\t\t\tfor i in range(len(row)-1):\n", 
        "\t\t\t\tcoef[i + 1] = coef[i + 1] + l_rate * error * yhat * (1.0 - yhat) * row[i]\n", 
        "\t\tprint('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n", 
        "\treturn coef\n"
      ], 
      "cell_type": "code", 
      "execution_count": null, 
      "outputs": [], 
      "metadata": {}
    }, 
    {
      "source": [
        "We will use a larger learning rate of 0.3 and train the model for 100 epochs, or 100 exposures\n", 
        "of the coefficients to the entire training dataset. Running the example prints a message each\n", 
        "epoch with the sum squared error for that epoch and the final set of coefficients."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "# Calculate coefficients\n", 
        "dataset = [[2.7810836,2.550537003,0],\n", 
        "\t[1.465489372,2.362125076,0],\n", 
        "\t[3.396561688,4.400293529,0],\n", 
        "\t[1.38807019,1.850220317,0],\n", 
        "\t[3.06407232,3.005305973,0],\n", 
        "\t[7.627531214,2.759262235,1],\n", 
        "\t[5.332441248,2.088626775,1],\n", 
        "\t[6.922596716,1.77106367,1],\n", 
        "\t[8.675418651,-0.242068655,1],\n", 
        "\t[7.673756466,3.508563011,1]]\n", 
        "l_rate = 0.3\n", 
        "n_epoch = 100\n", 
        "coef = coefficients_sgd(dataset, l_rate, n_epoch)\n", 
        "print(coef)\n"
      ], 
      "cell_type": "code", 
      "execution_count": null, 
      "outputs": [], 
      "metadata": {}
    }, 
    {
      "source": [
        "You can see how error continues to drop even in the final epoch. We could probably train for\n", 
        "a lot longer (more epochs) or increase the amount we update the coefficients each epoch (higher\n", 
        "learning rate)."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }
  ], 
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3", 
      "name": "python3", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "3.6.1", 
      "pygments_lexer": "ipython3", 
      "codemirror_mode": {
        "version": 3, 
        "name": "ipython"
      }
    }, 
    "anaconda-cloud": {}
  }
}